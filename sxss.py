#!/usr/bin/env python
# encoding: utf8
import argparse
import os
import sys
import re
import string
import urllib
import urlparse
import random
import requests
from bs4 import BeautifulSoup as bs
from pprint import pprint

import j_pentest_resource
import j_pentest_utils

# enumerator-like values used for marking current phase
GET, POST            = "GET", "POST"

# length of random prefix/suffix used in XSS tampering
PREFIX_SUFFIX_LENGTH = 8

# connection timeout in seconds
TIMEOUT = 3

def retrive_content( url, data='' ):
    headers = { 'user-agent': j_pentest_resource.pc_chrome_ua, }
    r = requests.get( url, headers=headers, timeout=TIMEOUT, allow_redirects=True )
    r = r.content.decode( r.encoding if r.encoding else 'utf8', 'ignore' )
    return r

def page_to_leaf( page ):
    def no_child_filter( tag ):
        if tag.find_all( True )==[]:
            return True
        else:
            return False
    soup = bs( page, 'lxml' )
    r = soup.find_all( no_child_filter )
    return r

def page_to_match_count( page, pattern ):
    count = 0
    for leaf in page_to_leaf( page ):
        for match in re.finditer( pattern, leaf.encode('utf8') ):
            count += 1
    return count

def match_index_to_leaf( page, pattern, match_index ):
    i = 0
    for leaf in page_to_leaf( page ):
        for jfdkfjdk in re.finditer( pattern, leaf.encode('utf8') ):
            i += 1
            if i-1 == match_index:
                return leaf
    else:
        print 'match_index ', match_index, 'not found'
        return None

def scan_page(url, data=None):

    retval = {}
    retval['url'] = url
    retval['payloads'] = []

    url, data = re.sub(r"=(&|\Z)", "=1\g<1>", url) if url else url, re.sub(r"=(&|\Z)", "=1\g<1>", data) if data else data

    tag = prefix = suffix = ''.join( random.sample( string.ascii_lowercase, PREFIX_SUFFIX_LENGTH ) )
    tag = prefix = suffix = 'IIIIIIII' # debug tag, easy to search

    #for phase in (GET, POST):
    for phase in (GET, ):

        current = url if phase is GET else (data or "")

        for match in re.finditer(r"((\A|[?&])(?P<parameter>[\w\[\]]+)=)(?P<value>[^&#]*)", current):
            
            found, usable = False, True
            context_list = []

            print "* scanning %s parameter '%s'" % (phase, match.group("parameter"))

            # baseline request
            tampered = current.replace( match.group(0), match.group(0) + tag )
            r = retrive_content( tampered )
            baseline_match_count = len( re.findall( tag, r ) )

            print 'baseline_match_count:', baseline_match_count
            if not baseline_match_count:# parameter not found in response, impossible to hack
                continue
            
            payloads = j_pentest_utils.textcsv_read( 'payloads.txt' )

            for payload in payloads:
                payload = '{tag}{rawpayload}{tag}'.format( tag=tag, rawpayload=payload )
                payload_pattern = r'{tag}(?P<payload_output>.*?){tag}'.format(tag=tag)
                payload_output_mutants = []

                tampered = current.replace( match.group(0), match.group(0) + payload )
                r = retrive_content( tampered )

                for payload_match in re.finditer( payload_pattern, r ):
                    payload_output_mutants.append( payload_match.groupdict()['payload_output'] )
                
                retval['payloads'].append( {
                    'parameter': match.group(0),
                    'rawpayload': payload,
                    'outputmutants': payload_output_mutants
                } )

    return retval

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-u", "--url", dest="url", required=True, help="Target URL (e.g. \"http://www.target.com/page.php?id=1\")")
    parser.add_argument("--data", dest="data", help="POST data (e.g. \"query=test\")")
    parser.add_argument("--cookie", dest="cookie", help="HTTP Cookie header value")
    parser.add_argument("--user-agent", dest="ua", default="Mozilla/5.0 (Windows NT 6.1; WOW64; rv:46.0) Gecko/20100101 Firefox/46.0", help="HTTP User-Agent header value")
    parser.add_argument("--referer", dest="referer", help="HTTP Referer header value")
    args = parser.parse_args()

    result = scan_page(args.url, args.data)
    pprint( result )
